{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport time\nimport numpy as np # linear algebra                                                                                                                                                                         \nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)                                                                                                                                      \nfrom tqdm import tqdm\nimport math\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\n\n\nimport matplotlib.pylab as plt","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"c8b534102c3e28e0c5578ae355bb89be87be6e1a"},"cell_type":"markdown","source":"**Load data**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_df = pd.read_csv(\"../input/train.csv\")\ntest_df = pd.read_csv(\"../input/test.csv\")\nprint(\"Train shape : \",train_df.shape)\nprint(\"Test shape : \",test_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ea0328ca5e2c0a6b8e3ca0bf9c179f4e5c4679d2"},"cell_type":"markdown","source":"**Setup cross validation and training dataset**"},{"metadata":{"trusted":true,"_uuid":"b324e9197f12398425a05bf5646464b7f3cd1787"},"cell_type":"code","source":"# Cross validation - create training and testing dataset\ntrain_df, val_df = train_test_split(train_df, test_size=0.1, random_state=2018)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1cbc4f8ce85bf4b92902f67a9882345e30d6d23b"},"cell_type":"markdown","source":"**Preprocess the data**"},{"metadata":{"trusted":true,"_uuid":"7fca31ab0c79809c06cf8897baf22021168e501a"},"cell_type":"code","source":"# Preprocess the data\n## some config values                                                                                                                                                                                       \nembed_size = 300 # how big is each word vector                                                                                                                                                              \nmax_features = 50000 # how many unique words to use (i.e num rows in embedding vector)                                                                                                                      \nmaxlen = 20 # max number of words in a question to use                                                                                                                                                     \n\n## fill up the missing values                                                                                                                                                                               \ntrain_X = train_df[\"question_text\"].fillna(\"_na_\").values\nval_X = val_df[\"question_text\"].fillna(\"_na_\").values\ntest_X = test_df[\"question_text\"].fillna(\"_na_\").values\n\n## Tokenize the sentences                                                                                                                                                                                   \ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_X))\ntrain_X = tokenizer.texts_to_sequences(train_X)\nval_X = tokenizer.texts_to_sequences(val_X)\ntest_X = tokenizer.texts_to_sequences(test_X)\n\n## Pad the sentences                                                                                                                                                                                        \ntrain_X = pad_sequences(train_X, maxlen=maxlen)\nval_X = pad_sequences(val_X, maxlen=maxlen)\ntest_X = pad_sequences(test_X, maxlen=maxlen)\n\n## Get the target values                                                                                                                                                                                    \ntrain_y = train_df['target'].values\nval_y = val_df['target'].values","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0d9d2e845e5140966ef28e65896c8b765285dd85"},"cell_type":"markdown","source":"**Build your model**"},{"metadata":{"trusted":true,"_uuid":"f0d062141c8dee77848a99c5ead476417924fb82"},"cell_type":"code","source":"EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n        \ninp = Input(shape=(maxlen,))\nx = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\nx = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\nx = GlobalMaxPool1D()(x)\nx = Dense(16, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(1, activation=\"sigmoid\")(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"0efd468651d7e17f6cde7aa268926216bf33aec3"},"cell_type":"markdown","source":"**Train  model**"},{"metadata":{"trusted":true,"_uuid":"c6e7fcb1276978d1d25ed46f189eee09d0edb149"},"cell_type":"code","source":"model.fit(train_X, train_y, batch_size=3000, epochs=2, validation_data=(val_X, val_y))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"9e4b24b6e3e90ef0aec9856ce6613d0e09192cee"},"cell_type":"markdown","source":"**Prediction on validation dataset**"},{"metadata":{"trusted":true,"_uuid":"30ed3f40a27e40d0f4be602e22da2c27d14283c9"},"cell_type":"code","source":"pred_noemb_val_y = model.predict([val_X], batch_size=1024, verbose=1)\n\nthresholds = np.arange(0.1, 0.501, 0.01)\nf1s = np.zeros(thresholds.shape[0])\n\nfor ind, thresh in np.ndenumerate(thresholds):\n    f1s[ind[0]] = metrics.f1_score(val_y, (pred_noemb_val_y > np.round(thresh, 2)).astype(int))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"35dead0062146130c9ab4a9c96f62f538ff23185"},"cell_type":"code","source":"np.round(thresholds[np.argmax(f1s)], 2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b2b9e9e8da99a0d5f1a3f228e032e71e60636271"},"cell_type":"code","source":"import itertools\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import svm, datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n    # print(\"Normalized confusion matrix\")\n    # else:\n    # print('Confusion matrix, without normalization')\n\n    # print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    # plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"208a100e0b28b06cf83ab3706c4f4aba0639836b"},"cell_type":"code","source":"opt_thresh = np.round(thresholds[np.argmax(f1s)], 2)\ny_test = val_y\ny_pred = (pred_noemb_val_y > opt_thresh).astype(int)\n\ncnf_matrix = confusion_matrix(y_test, y_pred)\n\nnp.set_printoptions(precision=2)\n\n# Plot non-normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=['Sincere','Insincere'],\n                      title='Confusion matrix, without normalization')\n\n# Plot normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=['Sincere','Insincere'], normalize=True,\n                      title='Normalized confusion matrix')\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ad2b9aafcdc2aab2de521539a10145b5b19c5a57"},"cell_type":"code","source":"precision = cnf_matrix[1,1]/(cnf_matrix[1,1]+cnf_matrix[0,1])\nrecall = cnf_matrix[1,1]/(cnf_matrix[1,1]+cnf_matrix[1,0])\nprint(\"Precision: \" + str(np.round(precision, 3)))\nprint(\"Recall: \" + str(np.round(recall, 3)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4da9708498c4eddbb6b7f49cd872d4b8d20a27bc"},"cell_type":"code","source":"# Next step is to look at some that are correct and incorrect","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b1a8c9dbec48d28d679c8f17489a7b3fc23ee379"},"cell_type":"code","source":"pred_noemb_test_y = model.predict([test_X], batch_size=1024, verbose=1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6610391ebe4e45c37d5c7f8c606a9862bd6d218e"},"cell_type":"code","source":"word_index = tokenizer.word_index\nval_df[\"question_text\"].fillna(\"_na_\").values ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c8d6816d581894a430b1cb711adb5ce5ee67d6bb"},"cell_type":"code","source":"original_text = val_df[\"question_text\"].fillna(\"_na_\").values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8582e6d6cf71fe9001b723bb023e83d6ba678546"},"cell_type":"code","source":"import operator\nfrom tqdm import tqdm\ndef analyze_model(model, num_results, reverse = False):\n    #let's see on which comments we get the biggest loss\n    train_predictions = model.predict([val_X], batch_size=250, verbose=1)\n    inverted_word_index = dict([[v,k] for k,v in word_index.items()])\n\n    results = []\n    eps = 0.1 ** 64\n    for i in tqdm(range(0, len(val_y))):\n        metric = 0\n\n        for j in range(len([val_y[i]])):\n            p = train_predictions[i][j]\n            y = [val_y[i]][j]\n            metric +=  -(y * math.log(p + eps) + (1 - y) * math.log(1 - p + eps))\n\n        results.append((original_text[i], metric, val_y[i], train_predictions[i], val_X[i]))\n    results.sort(key=operator.itemgetter(1), reverse=reverse)  \n\n    for i in range(num_results):\n        inverted_text = \"\"\n        for index in results[i][4]:\n            if index > 0:\n                word = inverted_word_index[index]\n                if not np.any(embedding_matrix[index]):\n                    word = \"_\" + word + \"_\"\n                inverted_text += word + \" \"\n\n\n        print(str(results[i][2]) + \"\\t\" + str(results[i][3]) + \"\\t\" + str(results[i][1]))\n        print(\"Original Text\")\n        print( str(results[i][0]))\n        print(\"---------------------------\")\n        print(\"Text that reached the model\")\n        print(inverted_text)\n        print(\"\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9820c605c1713989f1cd7d95f3bdbcbe58a5b60"},"cell_type":"code","source":"#500 highest loss comments\n#Correct Label | Model Output | Loss\n#Original Text\n#===========\n#Text that reached the model after preprocessing, tokenizing and embedding\nanalyze_model(model, 500, True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7390be03f886035a775476a97fc0ff0ca460b169"},"cell_type":"code","source":"analyze_model(model, 500, False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"baa9fd237f92480bb8cf8627281306f2537b74ce"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}