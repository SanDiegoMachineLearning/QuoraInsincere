{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np # linear algebra                                                                                                                                                                         \n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)                                                                                                                                      \n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "\n",
    "import matplotlib.pylab as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c8b534102c3e28e0c5578ae355bb89be87be6e1a"
   },
   "source": [
    "**Load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape :  (1306122, 3)\n",
      "Test shape :  (56370, 2)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"../input/train.csv\")\n",
    "test_df = pd.read_csv(\"../input/test.csv\")\n",
    "print(\"Train shape : \",train_df.shape)\n",
    "print(\"Test shape : \",test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ea0328ca5e2c0a6b8e3ca0bf9c179f4e5c4679d2"
   },
   "source": [
    "**Setup cross validation and training dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "b324e9197f12398425a05bf5646464b7f3cd1787"
   },
   "outputs": [],
   "source": [
    "# Cross validation - create training and testing dataset\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1cbc4f8ce85bf4b92902f67a9882345e30d6d23b"
   },
   "source": [
    "**Preprocess the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "7fca31ab0c79809c06cf8897baf22021168e501a"
   },
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "## some config values                                                                                                                                                                                       \n",
    "embed_size = 300 # how big is each word vector                                                                                                                                                              \n",
    "max_features = 50000 # how many unique words to use (i.e num rows in embedding vector)                                                                                                                      \n",
    "maxlen = 20 # max number of words in a question to use                                                                                                                                                     \n",
    "\n",
    "## fill up the missing values                                                                                                                                                                               \n",
    "train_X = train_df[\"question_text\"].fillna(\"_na_\").values\n",
    "val_X = val_df[\"question_text\"].fillna(\"_na_\").values\n",
    "test_X = test_df[\"question_text\"].fillna(\"_na_\").values\n",
    "\n",
    "## Tokenize the sentences                                                                                                                                                                                   \n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(train_X))\n",
    "train_X = tokenizer.texts_to_sequences(train_X)\n",
    "val_X = tokenizer.texts_to_sequences(val_X)\n",
    "test_X = tokenizer.texts_to_sequences(test_X)\n",
    "\n",
    "## Pad the sentences                                                                                                                                                                                        \n",
    "train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "val_X = pad_sequences(val_X, maxlen=maxlen)\n",
    "test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "\n",
    "## Get the target values                                                                                                                                                                                    \n",
    "train_y = train_df['target'].values\n",
    "val_y = val_df['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0d9d2e845e5140966ef28e65896c8b765285dd85"
   },
   "source": [
    "**Build your model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "f0d062141c8dee77848a99c5ead476417924fb82"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "        \n",
    "def get_model():\n",
    "    inp = Input(shape=(maxlen,))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dense(16, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "34c664e2e2daffe854336f938d31038e7cec092c"
   },
   "outputs": [],
   "source": [
    "models = {}\n",
    "# Initialize three models:\n",
    "# 0. Embedding layer always trainable\n",
    "# 1. Embedding layer trainable at beginning of training; frozen at end\n",
    "# 2. Embedding layer trainable at end of training; frozen at beginning\n",
    "models['Embedding_Always_Trainable'] = get_model()\n",
    "models['Embedding_Frozen_At_End'] = get_model()\n",
    "models['Embedding_Frozen_At_Start'] = get_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "0730719ba9036f14968d06664ab163e0f56f0380"
   },
   "outputs": [],
   "source": [
    "def set_embedding_trainable(model, trainable):\n",
    "    for layer in model.layers:\n",
    "        if layer.name.startswith('embedding'):\n",
    "            layer.trainable = trainable\n",
    "            print(\"Set '%s' layer trainable=%s\"%(layer.name, str(trainable)))\n",
    "            break\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "d4669b15c50c9cf3669e234d253618cce42f05f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set 'embedding_3' layer trainable=False\n"
     ]
    }
   ],
   "source": [
    "# Set Embedding layer 'Freeze_Embedding_At_Start' to trainable=False\n",
    "set_embedding_trainable(models['Embedding_Frozen_At_Start'], False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0efd468651d7e17f6cde7aa268926216bf33aec3"
   },
   "source": [
    "**Train  models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "c6e7fcb1276978d1d25ed46f189eee09d0edb149"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: Embedding_Always_Trainable...\n",
      "Train on 1175509 samples, validate on 130613 samples\n",
      "Epoch 1/2\n",
      "1175509/1175509 [==============================] - 24s 20us/step - loss: 0.1353 - acc: 0.9482 - val_loss: 0.1100 - val_acc: 0.9556\n",
      "Epoch 2/2\n",
      "1175509/1175509 [==============================] - 20s 17us/step - loss: 0.1054 - acc: 0.9579 - val_loss: 0.1070 - val_acc: 0.9568\n",
      "Training model: Embedding_Frozen_At_End...\n",
      "Train on 1175509 samples, validate on 130613 samples\n",
      "Epoch 1/2\n",
      "1175509/1175509 [==============================] - 21s 18us/step - loss: 0.1309 - acc: 0.9506 - val_loss: 0.1097 - val_acc: 0.9555\n",
      "Epoch 2/2\n",
      "1175509/1175509 [==============================] - 21s 18us/step - loss: 0.1053 - acc: 0.9580 - val_loss: 0.1066 - val_acc: 0.9568\n",
      "Training model: Embedding_Frozen_At_Start...\n",
      "Train on 1175509 samples, validate on 130613 samples\n",
      "Epoch 1/2\n",
      "1175509/1175509 [==============================] - 15s 13us/step - loss: 0.1407 - acc: 0.9478 - val_loss: 0.1166 - val_acc: 0.9532\n",
      "Epoch 2/2\n",
      "1175509/1175509 [==============================] - 14s 12us/step - loss: 0.1167 - acc: 0.9543 - val_loss: 0.1111 - val_acc: 0.9555\n"
     ]
    }
   ],
   "source": [
    "for key in models.keys():\n",
    "    print(\"Training model: %s...\"%key)\n",
    "    model = models.get(key)\n",
    "    model.fit(train_X, train_y, batch_size=3000, epochs=2, validation_data=(val_X, val_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "5d452707db4d443eb7d136119ee87f166cb98196"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set 'embedding_2' layer trainable=False\n",
      "Set 'embedding_3' layer trainable=True\n"
     ]
    }
   ],
   "source": [
    "# Set Embedding layer of 'Freeze_Embedding_At_End' trainable=False\n",
    "set_embedding_trainable(models['Embedding_Frozen_At_End'], False)\n",
    "\n",
    "# Set Embedding layer of 'Freeze_Embedding_At_Start' model trainable=True\n",
    "set_embedding_trainable(models['Embedding_Frozen_At_Start'], True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "c0b8161a4a510ade79cd85f91c700a042d5c3d68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Continue training model: Embedding_Always_Trainable...\n",
      "Train on 1175509 samples, validate on 130613 samples\n",
      "Epoch 1/2\n",
      "1175509/1175509 [==============================] - 21s 18us/step - loss: 0.0952 - acc: 0.9618 - val_loss: 0.1077 - val_acc: 0.9568\n",
      "Epoch 2/2\n",
      "1175509/1175509 [==============================] - 21s 18us/step - loss: 0.0849 - acc: 0.9662 - val_loss: 0.1115 - val_acc: 0.9562\n",
      "Continue training model: Embedding_Frozen_At_End...\n",
      "Train on 1175509 samples, validate on 130613 samples\n",
      "Epoch 1/2\n",
      "1175509/1175509 [==============================] - 15s 13us/step - loss: 0.0930 - acc: 0.9626 - val_loss: 0.1087 - val_acc: 0.9559\n",
      "Epoch 2/2\n",
      "1175509/1175509 [==============================] - 14s 12us/step - loss: 0.0899 - acc: 0.9641 - val_loss: 0.1095 - val_acc: 0.9564\n",
      "Continue training model: Embedding_Frozen_At_Start...\n",
      "Train on 1175509 samples, validate on 130613 samples\n",
      "Epoch 1/2\n",
      "1175509/1175509 [==============================] - 22s 18us/step - loss: 0.1096 - acc: 0.9567 - val_loss: 0.1056 - val_acc: 0.9574\n",
      "Epoch 2/2\n",
      "1175509/1175509 [==============================] - 21s 18us/step - loss: 0.0978 - acc: 0.9609 - val_loss: 0.1053 - val_acc: 0.9573\n"
     ]
    }
   ],
   "source": [
    "# Train for several more epochs\n",
    "for key in models.keys():\n",
    "    print(\"Continue training model: %s...\"%key)\n",
    "    model = models.get(key)\n",
    "    model.fit(train_X, train_y, batch_size=3000, epochs=2, validation_data=(val_X, val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9e4b24b6e3e90ef0aec9856ce6613d0e09192cee"
   },
   "source": [
    "**Prediction on validation dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "30ed3f40a27e40d0f4be602e22da2c27d14283c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130613/130613 [==============================] - 1s 7us/step\n",
      "130613/130613 [==============================] - 1s 7us/step\n",
      "130613/130613 [==============================] - 1s 7us/step\n"
     ]
    }
   ],
   "source": [
    "predictions = {}\n",
    "for key in models.keys():\n",
    "    model = models.get(key)\n",
    "    predictions[key] = model.predict([val_X], batch_size=1024, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "2be2d4adf42e0ab2f85c2bf8c73270e88640d9c8"
   },
   "outputs": [],
   "source": [
    "def tweak_threshold(pred, truth):\n",
    "    thresholds = []\n",
    "    scores = []\n",
    "    for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "        thresh = np.round(thresh, 2)\n",
    "        thresholds.append(thresh)\n",
    "        score = metrics.f1_score(truth, (pred>thresh).astype(int))\n",
    "        scores.append(score)\n",
    "    return np.max(scores), thresholds[np.argmax(scores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "c54bf36d6d314c350d051f5d8a2ed4e00ebfe795"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'Embedding_Always_Trainable' scored 0.6475 for threshold 0.26 on validation data\n",
      "Model 'Embedding_Frozen_At_End' scored 0.6455 for threshold 0.35 on validation data\n",
      "Model 'Embedding_Frozen_At_Start' scored 0.6578 for threshold 0.34 on validation data\n"
     ]
    }
   ],
   "source": [
    "for key in predictions.keys():\n",
    "    pred_val = predictions.get(key)\n",
    "    score_val, threshold_val = tweak_threshold(pred_val, val_y)\n",
    "    print(f\"Model '{key}' scored {round(score_val, 4)} for threshold {threshold_val} on validation data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
